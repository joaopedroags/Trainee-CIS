{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sumário\n",
        "\n",
        "O presente código apresenta uma Rede Neural utilizada para previsão em fraudes de cartões de crédito estruturada do \"zero\". Dito isso, os processos para realização foram os seguintes:\n",
        "\n",
        "1) Compreensão e análise do dataset: nesta etapa, notou-se que seriam utilizados dados numéricos (não havendo necessidade de codificação de variáveis) e que o dataset já estava equilibrado (todas as colunas com o mesmo número de linhas) destacando a inexistência de valores faltantes. O único problema exposto foi o desequilíbrio na distribuição da nossa variável alvo (Class), entrave esse que foi resolvido na etapa seguinte.\n",
        "\n",
        "2) Separando a label das features e divisão do dataset em dois subsets de treinamento e teste e treinamento e avaliação: inicialmente, dividiu-se as variáveis em inputs (independentes) e target (dependente) e normalizou a distribuição das primeiras. Após isso, foi criada a primeira instância de treino e teste (que foi utilizada posteriormente na hora da avaliação do modelo sob o conjunto de teste) para não enviesar a performance do modelo com os dados sintéticos que seriam utilizados para correção do *oversampling*. Após utilização do SMOTE, estruturou-se o segundo subset de treino e teste, esse que foi utilizado nos processos de treinamento e validação do modelo (antes das previsões no conjunto de teste).\n",
        "\n",
        "3) Definição da espinha dorsal da rede neural: aqui, foram criadas as classes (cada uma com suas respectivas funções) que estruturaram a rede neural, sendo elas:\n",
        "\n",
        "1. Layer: nesta classe foi definida a arquitetura das camadas da Rede Neural, estabelecendo processos como: inicialização, processamento de inputs e cálculo de gradientes com relação a perda.\n",
        "\n",
        "2. ActivationReLU e ActivationSigmoid: nestas duas classes foram definidas as funções de ativação dos neuronios que compoõem a nossa rede.\n",
        "\n",
        "3. LossBinaryCrossEntropy: definição e implementação da função de perda, responsável por medir a disparidade entre o que esta sendo previsto e o real, isto é, o quão distante a rede está da resposta real.\n",
        "\n",
        "\n",
        "4) Treinamento, validação e teste do modelo: Definição da função de treinamento e validação do modelo e, posteriormente, avaliação das previsões do modelo perante o conjunto de teste.\n",
        "\n",
        "Nesta etapa, foi definida também a arquitetura da rede neural, seguindo a seguinte estrutura:\n",
        "* Layer 1 (Input) : 31 neurônios;\n",
        "* Layer 2: 124 neurônios (Hidden Layer);\n",
        "* Layer 3: 2 neurônios;\n",
        "* Layer 4 (Output) : Resultado final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NfDaTfU3aacn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Ig9y724sEF",
        "outputId": "232c3890-4dcd-4c14-c1da-a20ba198381b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(284807, 31)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n",
            "Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = '/content/creditcard.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "# Verificando se existem valores nulos\n",
        "print(df.isnull().sum())\n",
        "print(df['Class'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota-se que há um desbalanceamento muito grande na nossa varíavel target"
      ],
      "metadata": {
        "id": "Cka8XzodA1Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Balanceando a varíavel target, estruturando e avaliando a Rede Neural"
      ],
      "metadata": {
        "id": "fpJ9P4yZA7Bm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPn977Fb8YjC",
        "outputId": "f455a2fa-c31c-4341-bd9c-5733f1750031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class\n",
            "0    284315\n",
            "1     85294\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Treinando com learning_rate=0.001\n",
            "Epoch 0: Loss = 0.5441\n",
            "Epoch 100: Loss = 0.0069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b2baee493055>:53: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-inputs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200: Loss = 0.0042\n",
            "Epoch 300: Loss = 0.0035\n",
            "Validation: Accuracy = 0.9994, Precision = 0.9976, Recall = 1.0000, F1 = 0.9988\n",
            "Previsões: [1 1 0 0 0 0 1 0 0 0]\n",
            "\n",
            "Treinando com learning_rate=0.005\n",
            "Epoch 0: Loss = 0.1207\n",
            "Epoch 100: Loss = 0.0031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b2baee493055>:53: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-inputs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200: Loss = 0.0028\n",
            "Epoch 300: Loss = 0.0029\n",
            "Validation: Accuracy = 0.9996, Precision = 0.9984, Recall = 1.0000, F1 = 0.9992\n",
            "Previsões: [1 1 0 0 0 0 1 0 0 0]\n",
            "\n",
            "Treinando com learning_rate=0.01\n",
            "Epoch 0: Loss = 0.1042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b2baee493055>:53: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-inputs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Loss = 0.0034\n",
            "Epoch 200: Loss = 0.0034\n",
            "Epoch 300: Loss = 0.0035\n",
            "Validation: Accuracy = 0.9996, Precision = 0.9983, Recall = 1.0000, F1 = 0.9991\n",
            "Previsões: [1 1 0 0 0 0 1 0 0 0]\n",
            "\n",
            "Treinando com learning_rate=0.05\n",
            "Epoch 0: Loss = 0.0218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b2baee493055>:53: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-inputs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Loss = 0.0030\n",
            "Epoch 200: Loss = 0.0027\n",
            "Epoch 300: Loss = 0.0028\n",
            "Validation: Accuracy = 0.9995, Precision = 0.9979, Recall = 1.0000, F1 = 0.9990\n",
            "Previsões: [1 1 0 0 0 0 1 0 0 0]\n",
            "\n",
            "Treinando com learning_rate=0.1\n",
            "Epoch 0: Loss = 0.0136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b2baee493055>:53: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-inputs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Loss = 3.7280\n",
            "Epoch 200: Loss = 3.7280\n",
            "Epoch 300: Loss = 3.2464\n",
            "Validation: Accuracy = 0.7986, Precision = 0.5372, Recall = 0.9322, F1 = 0.6816\n",
            "Previsões: [1 1 0 0 0 0 1 0 1 0]\n",
            "\n",
            "Treinando com learning_rate=0.15\n",
            "Epoch 0: Loss = 0.2169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b2baee493055>:53: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-inputs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Loss = 0.1754\n",
            "Epoch 200: Loss = 0.1754\n",
            "Epoch 300: Loss = 0.1754\n",
            "Validation: Accuracy = 0.9557, Precision = 0.9986, Recall = 0.8098, F1 = 0.8944\n",
            "Previsões: [1 1 0 0 0 0 1 0 0 0]\n",
            "\n",
            "Treinando com learning_rate=0.2\n",
            "Epoch 0: Loss = 0.0185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b2baee493055>:53: RuntimeWarning: overflow encountered in exp\n",
            "  self.output = 1 / (1 + np.exp(-inputs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Loss = 3.7280\n",
            "Epoch 200: Loss = 3.7280\n",
            "Epoch 300: Loss = 3.7280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: Accuracy = 0.7687, Precision = 0.0000, Recall = 0.0000, F1 = 0.0000\n",
            "Previsões: [0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "Resultados finais no conjunto de teste:\n",
            "Accuracy = 0.9997, Precision = 0.8632, Recall = 1.0000, F1 = 0.9266\n",
            "Previsões finais: [1 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn import under_sampling, over_sampling\n",
        "\n",
        "# Separando a label das features\n",
        "inputs = df.drop('Class', axis=1)\n",
        "targets = df['Class']\n",
        "# Normalizando as varíaveis independentes\n",
        "x = inputs\n",
        "scaler = StandardScaler()\n",
        "x_new = scaler.fit_transform(x)\n",
        "y = targets\n",
        "\n",
        "# Dividindo o dataset em treino e teste (primeiro subset)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size=0.5, random_state=42)\n",
        "\n",
        "smote = SMOTE(sampling_strategy = 0.3, random_state=42)\n",
        "x_resampled, y_resampled = smote.fit_resample(x_new, y)\n",
        "\n",
        "# Verificando a nova distirbuição da variavel alvo\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "# Dvidindo o dataset em treino e validação após o tratamento do oversampling com SMOTE\n",
        "X_train_resampled, X_val_resampled, y_train_resampled, y_val_resampled = train_test_split(x_resampled, y_resampled, test_size=0.25, random_state=42)\n",
        "\n",
        "# Definição da \"espinha dorsal\" da Rede Neural\n",
        "class Layer:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "      # Incializacação aleatória dos pesos\n",
        "        self.weights = np.random.randn(n_inputs, n_neurons) * 0.01\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, dvalues, learning_rate):\n",
        "        dvalues = dvalues.reshape(-1, 1) if len(dvalues.shape) == 1 else dvalues\n",
        "        self.dweights = np.dot(self.input.T, dvalues) / self.input.shape[0]\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True) / self.input.shape[0]\n",
        "        self.weights -= learning_rate * self.dweights\n",
        "        self.biases -= learning_rate * self.dbiases\n",
        "        self.dinput = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "class ActivationReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinput = dvalues.copy()\n",
        "        self.dinput[self.output <= 0] = 0\n",
        "\n",
        "class ActivationSigmoid:\n",
        "    def forward(self, inputs):\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinput = dvalues * (self.output * (1 - self.output))\n",
        "\n",
        "class LossBinaryCrossEntropy:\n",
        "    def calculate(self, outputs, targets):\n",
        "        outputs = np.clip(outputs, 1e-7, 1 - 1e-7)\n",
        "        return -np.mean(targets * np.log(outputs) + (1 - targets) * np.log(1 - outputs))\n",
        "\n",
        "    def backward(self, outputs, targets):\n",
        "        outputs = np.clip(outputs, 1e-7, 1 - 1e-7)\n",
        "        return (outputs - targets) / (outputs * (1 - outputs))\n",
        "\n",
        "# Definindo a função de Treinamento e Avaliação, bem como a estrutura da rede neural\n",
        "def train_and_evaluate(X_train_resampled, y_train_resampled, X_val_resampled, y_val_resampled, learning_rate, epochs, batch_size):\n",
        "    dense1 = Layer(X_train_resampled.shape[1], 124)\n",
        "    activation1 = ActivationReLU()\n",
        "    dense2 = Layer(124, 2)\n",
        "    activation2 = ActivationReLU()\n",
        "    dense3 = Layer(2, 1)\n",
        "    activation3 = ActivationSigmoid()\n",
        "    loss_function = LossBinaryCrossEntropy()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for start in range(0, X_train_resampled.shape[0], batch_size):\n",
        "            batch_X = X_train_resampled[start:start + batch_size]\n",
        "            batch_y = y_train_resampled.iloc[start:start + batch_size].values.reshape(-1, 1)\n",
        "\n",
        "            dense1.forward(batch_X)\n",
        "            activation1.forward(dense1.output)\n",
        "            dense2.forward(activation1.output)\n",
        "            activation2.forward(dense2.output)\n",
        "            dense3.forward(activation2.output)\n",
        "            activation3.forward(dense3.output)\n",
        "\n",
        "            loss_value = loss_function.calculate(activation3.output, batch_y)\n",
        "            dvalues = loss_function.backward(activation3.output, batch_y)\n",
        "            activation3.backward(dvalues)\n",
        "            dense3.backward(activation3.dinput, learning_rate)\n",
        "            activation2.backward(dense3.dinput)\n",
        "            dense2.backward(activation2.dinput, learning_rate)\n",
        "            activation1.backward(dense2.dinput)\n",
        "            dense1.backward(activation1.dinput, learning_rate)\n",
        "\n",
        "        # Avaliar no conjunto de validação\n",
        "        dense1.forward(X_val_resampled)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "        dense3.forward(activation2.output)\n",
        "        activation3.forward(dense3.output)\n",
        "\n",
        "        val_loss = loss_function.calculate(activation3.output, y_val_resampled.values.reshape(-1, 1))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss = {val_loss:.4f}\")\n",
        "\n",
        "    return dense1, activation1, dense2, activation2, dense3, activation3\n",
        "\n",
        "# Configurando o número de epochs (épocas), learning rate e batch size\n",
        "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, 0.2]\n",
        "epochs = 400\n",
        "batch_size = 32\n",
        "best_model = None\n",
        "best_f1 = 0\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nTreinando com learning_rate={lr}\")\n",
        "    model = train_and_evaluate(X_train_resampled, y_train_resampled, X_val_resampled, y_val_resampled, lr, epochs, batch_size)\n",
        "\n",
        "    dense1, activation1, dense2, activation2, dense3, activation3 = model\n",
        "\n",
        "    # Avaliar no conjunto de validação\n",
        "    dense1.forward(X_val_resampled)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    activation2.forward(dense2.output)\n",
        "    dense3.forward(activation2.output)\n",
        "    activation3.forward(dense3.output)\n",
        "\n",
        "    y_pred_val = (activation3.output >= 0.5).astype(int)\n",
        "    accuracy_val = accuracy_score(y_val_resampled, y_pred_val)\n",
        "    precision_val = precision_score(y_val_resampled, y_pred_val)\n",
        "    recall_val = recall_score(y_val_resampled, y_pred_val)\n",
        "    f1_val = f1_score(y_val_resampled, y_pred_val)\n",
        "\n",
        "    print(f\"Validation: Accuracy = {accuracy_val:.4f}, Precision = {precision_val:.4f}, Recall = {recall_val:.4f}, F1 = {f1_val:.4f}\")\n",
        "\n",
        "    # Printando previsões (no conjunto de validação) para auditoria\n",
        "    print(f\"Previsões: {y_pred_val.flatten()[:10]}\")\n",
        "\n",
        "    # Selecionando o modelo que obteve melhor performance no treinamento\n",
        "    if f1_val > best_f1:\n",
        "        best_f1 = f1_val\n",
        "        best_model = model\n",
        "\n",
        "# Prever resultados, no modelo de teste, utilizando o modelo com melhor performance\n",
        "dense1, activation1, dense2, activation2, dense3, activation3 = best_model\n",
        "\n",
        "dense1.forward(x_test)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "dense3.forward(activation2.output)\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "y_pred_test = (activation3.output >= 0.5).astype(int)\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "precision_test = precision_score(y_test, y_pred_test)\n",
        "recall_test = recall_score(y_test, y_pred_test)\n",
        "f1_test = f1_score(y_test, y_pred_test)\n",
        "# Avaliando a performance\n",
        "print(\"\\nResultados finais no conjunto de teste:\")\n",
        "print(f\"Accuracy = {accuracy_test:.4f}, Precision = {precision_test:.4f}, Recall = {recall_test:.4f}, F1 = {f1_test:.4f}\")\n",
        "print(f\"Previsões finais: {y_pred_test.flatten()[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação dos resultados\n",
        "\n",
        "Antes de analisarmos a performance, é válido ressaltar os respectivos parâmetros utilizados no treinamento:\n",
        "1. Número de épocas: 400;\n",
        "2. Learning rate: 0.001, 0.005, 0.01, 0.05, 0.1, 0.15, 0.2;\n",
        "3. Batch size: 32\n",
        "\n",
        "Agora, vamos analisar cada cenário:\n",
        "\n",
        "* Treinamento com Learning rate = 0.001 e learning rate = 0.005;\n",
        "\n",
        "Notou-se não somente um acentuado decréscimo na taxa de perda (com poucas iterações) como também um elevado valor para todas as métricas de performance, caracterizando um possível overfitting.\n",
        "\n",
        "* Treinamento com Learning rate = 0.01 e Learning rate = 0.05;\n",
        "\n",
        "Novamente, nota-se um comportamento análogo ao caso anterior (baixas taxas de perda e altos valores para as métricas de avaliação), caracterizando também um possível overfitting.\n",
        "\n",
        "* Treinamento com Learning rate = 0.1;\n",
        "\n",
        "Aqui, apresenta-se um cenário tipico de underfitting haja vista que, o modelo apresenta uma elevação na taxa de perda ao longo das iterações e baixos valores em métricas como precisão e f1, indicando que o modelo está cometendo muitos erros de previsão.\n",
        "\n",
        "* Treinamento com Learning rate = 0.15;\n",
        "\n",
        "Neste cenário, não há evidências claras de overfitting (como nos exemplos anteriores) mesmo com a boa performance do modelo, o que já o descaracterizaria como underfitting.\n",
        "\n",
        "* Treinamento com Learning Rate = 0.2;\n",
        "\n",
        "Em específico, neste caso, o modelo apresentou sérias dificuldades para se ajustar aos dados, fato esse que fez com que esse treinamento fosse descartado da análise.\n",
        "\n",
        "Por fim, tomou-se o modelo com o maior F1-score ao longo do treinamento e o avaliou com relação às análises no conjunto de teste (que não continha amostras sintéticas). Mediante a análise das métricas de avaliação, pode-se concluir que o modelo apresentou um resultado satisfatório nas previsões realizadas.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kzL94uISm0N7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}